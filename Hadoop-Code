
To do spark streaming analysis using spark shell

spark-shell --conf spark.ui.port=11111 --packages org.apache.spark:spark-streaming-kafka_2.10:1.6.2

scala> sc.stop / sc.stop()

import kafka.serializer.StringDecoder
import org.apache.spark.SparkConf
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming._
import org.apache.spark.streaming.kafka._
import org.apache.spark.streaming.dstream.InputDStream

    val sparkConf = new SparkConf().setAppName("DepartmentWiseCount").setMaster("local")
    val topicsSet = "topictest1".split(",").toSet 
    val kafkaParams = Map[String,String]("metadata.broker.list" -> "nn01.itversity.com:6667,nn02.itversity.com:6667,rm01.itversity.com:6667")
    val ssc = new StreamingContext(sparkConf, Seconds(60)) 
    val messages = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc,kafkaParams, topicsSet)
    val lines = messages.map(_._2) 
    val linesFiltered = lines.filter(rec => rec.contains("GET/department/")) 
    val countByDepartment = linesFiltered.map(rec => (rec.split(" ")(6).split("/")(2), 1)).reduceByKey(_+ _) 
    // reduceByKeyAndWindow((a:Int, b:Int) => (a + b), Seconds(300), Seconds(60)) 
    // countByDepartment.print()
    countByDepartment.print() 
    ssc.start()
